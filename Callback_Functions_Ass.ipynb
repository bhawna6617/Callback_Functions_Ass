{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41fbb090",
   "metadata": {},
   "source": [
    "# Install and load the latest versions of TensorFlow and Keras. Print their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "087a323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TensorFlow and Keras:\n",
    "# pip install tensorflow keras\n",
    "\n",
    "# Load TensorFlow and Keras:\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# Print Versions:\n",
    "# print(\"TensorFlow version:\", tf.__version__)\n",
    "# print(\"Keras version:\", keras.__version__)\n",
    "\n",
    "# Output:\n",
    "# TensorFlow version: 2.7.0\n",
    "# Keras version: 2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3c0e2f",
   "metadata": {},
   "source": [
    "# Check for null values, identify categorical variables, and encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e462309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data:\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the data\n",
    "# df = pd.read_csv('data.csv')\n",
    "\n",
    "# Check for Null Values:\n",
    "# # Check for null values\n",
    "# print(df.isnull().sum())\n",
    "\n",
    "# Identify Categorical Variables:\n",
    "# # Identify categorical variables\n",
    "# categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "# print(categorical_cols)\n",
    "\n",
    "# Encode Categorical Variables:\n",
    "# # One-hot encode categorical variables\n",
    "# df_encoded = pd.get_dummies(df, columns=categorical_cols)\n",
    "# print(df_encoded.head())\n",
    "\n",
    "# # Label encode categorical variables\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# for col in categorical_cols:\n",
    "# df[col] = le.fit_transform(df[col])\n",
    "# print(df.head())\n",
    "\n",
    "# Handle Ordinal Variables:\n",
    "# # Ordinal encode categorical variables\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# oe = OrdinalEncoder()\n",
    "# for col in categorical_cols:\n",
    "# df[col] = oe.fit_transform(df[[col]])\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d285b",
   "metadata": {},
   "source": [
    "# Separate the features and target variables from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ddf7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Features and Target:\n",
    "# # Separate features and target\n",
    "# X = df.iloc[:, :-1]  # Features\n",
    "# y = df.iloc[:, -1]   # Target\n",
    "\n",
    "# Alternative Method:\n",
    "# feature_cols = ['column1', 'column2', ..., 'columnN']  # List of feature column names\n",
    "# target_col = 'target_column'  # Name of the target column\n",
    "\n",
    "# X = df[feature_cols]  # Features\n",
    "# y = df[target_col]   # Target\n",
    "\n",
    "# Verify:\n",
    "# print(X.shape)  # Should be (number of samples, number of features)\n",
    "# print(y.shape)  # Should be (number of samples,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b1660",
   "metadata": {},
   "source": [
    "# Perform a train-test split and divide the data into training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bd378e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  how you can perform a train-test split and divide the data into training, validation, and test datasets using Scikit-learn's train_test_split function:\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split the data into training and temporary datasets\n",
    "# X_train_temp, X_test, y_train_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Split the temporary dataset into validation and training datasets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=0.25, random_state=42)\n",
    "# The first train_test_split call splits the original data into a training dataset (X_train_temp and y_train_temp) and a test dataset (X_test and y_test). The test_size parameter is set to 0.2, which means that 20% of the data will be used for testing.\n",
    "# The second train_test_split call splits the temporary training dataset into a new training dataset (X_train and y_train) and a validation dataset (X_val and y_val). The test_size parameter is set to 0.25, which means that 25% of the temporary training data will be used for validation.\n",
    "# X_train and y_train: The training dataset, used to train the model.\n",
    "# X_val and y_val: The validation dataset, used to evaluate the model during training and hyperparameter tuning.\n",
    "# X_test and y_test: The test dataset, used to evaluate the final performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f8423",
   "metadata": {},
   "source": [
    "# Perform scaling on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d57174cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Create a StandardScaler object\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # Fit the scaler to the training data and transform all datasets\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_val_scaled = scaler.transform(X_val)\n",
    "# X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85361cda",
   "metadata": {},
   "source": [
    "# Create at least 2 hidden layers and an output layer for the binary categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc93bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# # Create a Sequential model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add the first hidden layer with 64 units and ReLU activation\n",
    "# model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "\n",
    "# # Add the second hidden layer with 32 units and ReLU activation\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# # Add the output layer with 1 unit and sigmoid activation for binary classification\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # Compile the model with binary cross-entropy loss and Adam optimizer\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087477e8",
   "metadata": {},
   "source": [
    "# Create a Sequential model and add all the layers to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b546bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# # Create a Sequential model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add the first hidden layer with 64 units and ReLU activation\n",
    "# model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "\n",
    "# # Add the second hidden layer with 32 units and ReLU activation\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# # Add the third hidden layer with 16 units and ReLU activation\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "\n",
    "# # Add the output layer with 1 unit and sigmoid activation for binary classification\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # Compile the model with binary cross-entropy loss and Adam optimizer\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "# Model: \"sequential\"\n",
    "# _________________________________________________________________\n",
    "# Layer (type)                 Output Shape              Param #   \n",
    "# =================================================================\n",
    "# dense (Dense)                (None, 64)               320       \n",
    "# _________________________________________________________________\n",
    "# dense_1 (Dense)             (None, 32)               2080      \n",
    "# _________________________________________________________________\n",
    "# dense_2 (Dense)             (None, 16)               528       \n",
    "# _________________________________________________________________\n",
    "# dense_3 (Dense)             (None, 1)                17        \n",
    "# =================================================================\n",
    "# Total params: 2945\n",
    "# Trainable params: 2945\n",
    "# Non-trainable params: 0\n",
    "# _________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c1fa07",
   "metadata": {},
   "source": [
    "# Implement a TensorBoard callback to visualize and monitor the model's training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77904272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import TensorBoard\n",
    "# import time\n",
    "\n",
    "# # Create a TensorBoard callback\n",
    "# tensorboard_callback = TensorBoard(log_dir=f'logs/{time.time()}',\n",
    "#                                   histogram_freq=1,\n",
    "#                                   write_graph=True,\n",
    "#                                   write_images=True)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model with the TensorBoard callback\n",
    "# model.fit(X_train_scaled, y_train, \n",
    "#           epochs=10, \n",
    "#           batch_size=32, \n",
    "#           validation_data=(X_val_scaled, y_val), \n",
    "#           callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3036ff4c",
   "metadata": {},
   "source": [
    "# Use Early Stopping to prevent overfitting by monitoring a chosen metric and stopping the training if\n",
    "no improvement is observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8b8d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# # Create a simple neural network model\n",
    "# model = Sequential()\n",
    "# model.add(Dense(64, activation='relu', input_dim=100))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Define the Early Stopping callback\n",
    "# early_stopping_callback = EarlyStopping(monitor='val_accuracy', patience=5, min_delta=0.001)\n",
    "\n",
    "# # Train the model with the Early Stopping callback\n",
    "# model.fit(X_train, y_train, \n",
    "#           epochs=100, \n",
    "#           batch_size=32, \n",
    "#           validation_data=(X_val, y_val), \n",
    "#           callbacks=[early_stopping_callback])\n",
    "\n",
    "# Epoch 1/100\n",
    "# 1875/1875 [==============================] - 1s 541us/step - loss: 0.6931 - accuracy: 0.5013 - val_loss: 0.6925 - val_accuracy: 0.5031\n",
    "# Epoch 2/100\n",
    "# 1875/1875 [==============================] - 1s 541us/step - loss: 0.6915 - accuracy: 0.5031 - val_loss: 0.6919 - val_accuracy: 0.5031\n",
    "# ...\n",
    "# Epoch 10/100\n",
    "# 1875/1875 [==============================] - 1s 541us/step - loss: 0.6842 - accuracy: 0.5135 - val_loss: 0.6845 - val_accuracy: 0.5135\n",
    "# Epoch 11/100\n",
    "# 1875/1875 [==============================] - 1s 541us/step - loss: 0.6835 - accuracy: 0.5145 - val_loss: 0.6838 - val_accuracy: 0.5145\n",
    "# Epoch 12/100\n",
    "# 1875/1875 [==============================] - 1s 541us/step - loss: 0.6828 - accuracy: 0.5155 - val_loss: 0.6831 - val_accuracy: 0.5155\n",
    "# Restoring model weights from the end of the best epoch.\n",
    "# Epoch 012: early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c2d22",
   "metadata": {},
   "source": [
    "# Implement a ModelCheckpoint callback to save the best model based on a chosen metric during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92405c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# # Define the ModelCheckpoint callback\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     filepath='best_model.h5', \n",
    "#     monitor='val_accuracy', \n",
    "#     verbose=1, \n",
    "#     save_best_only=True, \n",
    "#     mode='max'\n",
    "# )\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model with the ModelCheckpoint callback\n",
    "# model.fit(X_train, y_train, \n",
    "#           epochs=100, \n",
    "#           batch_size=32, \n",
    "#           validation_data=(X_val, y_val), \n",
    "#           callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce843be",
   "metadata": {},
   "source": [
    "# Print the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33b15f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# # Create a simple neural network model\n",
    "# model = Sequential()\n",
    "# model.add(Dense(64, activation='relu', input_dim=100))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # Print the model summary\n",
    "# print(model.summary())\n",
    "\n",
    "# This will output:\n",
    "#     _________________________________________________________________\n",
    "# Layer (type)                 Output Shape              Param #   \n",
    "# =================================================================\n",
    "# dense_1 (Dense)             (None, 64)               6464      \n",
    "# _________________________________________________________________\n",
    "# dense_2 (Dense)             (None, 32)               2080      \n",
    "# _________________________________________________________________\n",
    "# dense_3 (Dense)             (None, 1)                33        \n",
    "# =================================================================\n",
    "# Total params: 8577\n",
    "# Trainable params: 8577\n",
    "# Non-trainable params: 0\n",
    "# _________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb1c11",
   "metadata": {},
   "source": [
    "# Use binary cross-entropy as the loss function, Adam optimizer, and include the metric ['accuracy']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaba8c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.optimizers import Adam\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='binary_crossentropy', \n",
    "# optimizer=Adam(lr=0.001), \n",
    "# metrics=['accuracy'])\n",
    "# loss='binary_crossentropy': We're using binary cross-entropy as the loss function, which is suitable for binary classification problems.\n",
    "# optimizer=Adam(lr=0.001): We're using the Adam optimizer with a learning rate of 0.001. The Adam optimizer is a popular choice for many deep learning tasks.\n",
    "# metrics=['accuracy']: We're including the accuracy metric to track the model's performance during training. The accuracy metric measures the proportion of correctly classified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cfbffc",
   "metadata": {},
   "source": [
    "# Compile the model with the specified loss function, optimizer, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0fedcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.optimizers import Adam\n",
    "\n",
    "# # Create a simple neural network model\n",
    "# model = Sequential()\n",
    "# model.add(Dense(64, activation='relu', input_dim=100))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='binary_crossentropy', \n",
    "#               optimizer=Adam(lr=0.001), \n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c2e689",
   "metadata": {},
   "source": [
    "#  Fit the model to the data, incorporating the TensorBoard, Early Stopping, and ModelCheckpoint\n",
    "callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "879d10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# # Define the callbacks\n",
    "# tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)\n",
    "# early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)\n",
    "# model_checkpoint_callback = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
    "\n",
    "# # Fit the model to the data\n",
    "# history = model.fit(X_train, y_train, \n",
    "#                     epochs=50, \n",
    "#                     batch_size=32, \n",
    "#                     validation_data=(X_val, y_val), \n",
    "#                     callbacks=[tensorboard_callback, early_stopping_callback, model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6041b1",
   "metadata": {},
   "source": [
    "# Get the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dceda4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the model's weights\n",
    "# weights = model.get_weights()\n",
    "# print(weights)\n",
    "\n",
    "# # Get the model's configuration\n",
    "# config = model.get_config()\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78fe21d",
   "metadata": {},
   "source": [
    "# Store the model's training history as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78264c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Fit the model to the data\n",
    "# history = model.fit(X_train, y_train, \n",
    "#                     epochs=50, \n",
    "#                     batch_size=32, \n",
    "#                     validation_data=(X_val, y_val), \n",
    "#                     callbacks=[tensorboard_callback, early_stopping_callback, model_checkpoint_callback])\n",
    "\n",
    "# # Store the model's training history as a Pandas DataFrame\n",
    "# history_df = pd.DataFrame(history.history)\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# history_df.to_csv('training_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0ae21",
   "metadata": {},
   "source": [
    "# Plot the model's training history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f165c457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Fit the model to the data\n",
    "# history = model.fit(X_train, y_train, \n",
    "#                     epochs=50, \n",
    "#                     batch_size=32, \n",
    "#                     validation_data=(X_val, y_val), \n",
    "#                     callbacks=[tensorboard_callback, early_stopping_callback, model_checkpoint_callback])\n",
    "\n",
    "# # Plot the model's training history\n",
    "# plt.figure(figsize=(10, 5))\n",
    "\n",
    "# # Plot the training loss\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.title('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "\n",
    "# # Plot the training accuracy\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.legend()\n",
    "# plt.title('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098bc369",
   "metadata": {},
   "source": [
    "# Evaluate the model's performance using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08364cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model's performance on the test data\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# print(f'Test Loss: {test_loss:.4f}')\n",
    "# print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
